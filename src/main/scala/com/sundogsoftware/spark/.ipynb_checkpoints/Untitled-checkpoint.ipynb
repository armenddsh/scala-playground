{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd66836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9106e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize the Spark session\n",
    "logger.info(\"Starting Spark session\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Python Spark Home Property\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.master\", \"local\") \\\n",
    "    .config(\"spark.driver.memory\", \"20g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"4g\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"1000\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# Define the file paths\n",
    "home_property_path = \"C:/Users/armen/Desktop/HomeProperty\"\n",
    "base_filename = \"Property_202503_test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c537293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the input CSV files\n",
    "logger.info(\"Reading input CSV files\")\n",
    "df_home_property = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"{home_property_path}/{base_filename}\")\n",
    "logger.info(\"Read Property_202503_test.csv successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean the data\n",
    "cleaned_df = df_home_property.withColumnRenamed(\"PROPERTY INDICATOR CODE\", \"PROPERTY_INDICATOR_CODE\") \\\n",
    "    .filter(F.col(\"PROPERTY_INDICATOR_CODE\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the Data Dictionary CSV\n",
    "df_data_dictionary = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"{home_property_path}/Property_DataDictionary.csv\")\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = cleaned_df.join(df_data_dictionary, F.col(\"Property Indicator\") == F.col(\"PROPERTY_INDICATOR_CODE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23407016",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get distinct values of PROPERTY_INDICATOR_CODE\n",
    "logger.info(\"Processing each PROPERTY_INDICATOR_CODE value\")\n",
    "\n",
    "distinct_values = df_data_dictionary.select(\"Property Indicator\").distinct() \\\n",
    "    .rdd.map(lambda row: row[0]) \\\n",
    "    .filter(lambda x: x is not None and x.strip() != '') \\\n",
    "    .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger.info(f\"Distinct PROPERTY_INDICATOR_CODE values: {distinct_values}\")\n",
    "\n",
    "# Process each distinct value and write the results\n",
    "for property_indicator_code in distinct_values:\n",
    "    try:\n",
    "        output_path = f\"{home_property_path}/property_indicator_codes/property_indicator_code_{property_indicator_code}\"\n",
    "        logger.info(f\"Writing data for PROPERTY_INDICATOR_CODE = {property_indicator_code} to: {output_path}\")\n",
    "\n",
    "        filtered_df = joined_df.filter(F.col(\"PROPERTY_INDICATOR_CODE\") == property_indicator_code)\n",
    "        filtered_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "\n",
    "        logger.info(f\"Finished writing data for PROPERTY_INDICATOR_CODE = {property_indicator_code}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {property_indicator_code}: {e}\")\n",
    "\n",
    "logger.info(\"Finished processing all PROPERTY_INDICATOR_CODE values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef068e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stop the Spark session\n",
    "logger.info(\"Stopping Spark session\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
